{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothing images classification using Tensorflow/Keras\n",
    "\n",
    "Material adopted from [Tensorflow tutorials](https://www.tensorflow.org/tutorials/keras/classification)\n",
    "\n",
    "In this workflow we train a neural network to classify images of clothing (sneakers and shirts) using the high level tensroflow keras API. The entire training process can be broken down into a sequence of steps described as follows\n",
    "\n",
    "1. Import and generate the fashion dataset\n",
    "2. Preprocess the datasets: Scale the values in the images to range between [0, 1]\n",
    "3. Build and compile the model (Keras)\n",
    "4. Cross validate the model on the training set\n",
    "5. Optimize hyper-parameters (Regularization, Dropout rates ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and tf.keras\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from covalent.executor import SlurmExecutor, SSHExecutor\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import ParameterSampler, cross_val_score\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "import covalent as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and load the Fashion dataset'\n",
    "\n",
    "* [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) contains 70,000 grayscale images in 10 different categories\n",
    "* Each image is a 28 x 28 pixels in size\n",
    "* Each category in the image is label by an integer between 0 and 9\n",
    "\n",
    "| Label | Category |\n",
    "| ------|----------|\n",
    "| 0     | T-shirt/top |\n",
    "| 1     | Trouser |\n",
    "| 2     | Pullover |\n",
    "| 3     | Dress |\n",
    "| 4     | Coat |\n",
    "| 5     | Sandal |\n",
    "| 6     | Shirt |\n",
    "| 7     | Sneaker |\n",
    "| 8     | Bag |\n",
    "| 9     | Ankle Boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "    return (\n",
    "        train_images[:10000],\n",
    "        train_labels[:10000],\n",
    "        test_images[:1000],\n",
    "        test_labels[:1000],\n",
    "    )\n",
    "\n",
    "\n",
    "train_images, train_labels, test_images, test_labels = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "\n",
    "Scale all pixel values to range between [0 - 1]\n",
    "\n",
    "Lets inspect a single image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def scale_dataset(train_images, test_images):\n",
    "    return train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_images, scaled_test_images = scale_dataset(train_images, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(scaled_train_images[0], cmap=plt.cm.binary)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Keras model\n",
    "\n",
    "We use this electron to abstract building/compiling of the keras model given the three required input parameters\n",
    "\n",
    "* `hidden_layer_width`\n",
    "    * Integer describing the width of the single hidden layer\n",
    "    \n",
    "* `dropout_rate`\n",
    "    * float between [0, 1] marking the dropout rate for the network\n",
    "    \n",
    "* `l2_penalty`\n",
    "    * L2 regularization rate to prevent over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the remote executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_executor = SlurmExecutor(\n",
    "    username=\"venkat\",\n",
    "    address=\"beehive.agnostiq.ai\",\n",
    "    remote_workdir=\"/federation/venkat/workdir\",\n",
    "    ssh_key_file=\"/home/venkat/.ssh/id_ed25519\",\n",
    "    poll_freq=10,\n",
    "    conda_env=\"ieee\",\n",
    "    options={\n",
    "        \"partition\": \"debug\",\n",
    "        \"ntasks\": 1,\n",
    "        \"cpus-per-task\": 2,\n",
    "        \"chdir\": \"/federation/venkat/workdir\",\n",
    "        \"nodelist\": \"beehive-debug-st-t2medium-1\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "ssh_executor = SSHExecutor(\n",
    "    username=\"venkat\",\n",
    "    hostname=\"testbed.balavk.net\",\n",
    "    remote_cache_dir=\"/home/venkat/workdir\",\n",
    "    ssh_key_file=\"/home/venkat/.ssh/id_ed25519\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def build_keras_classifier(\n",
    "    hidden_layer_width: int,\n",
    "    dropout_rate: float,\n",
    "    l2_penalty: float,\n",
    "    optimizer: str = \"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    "):\n",
    "    \"\"\"Build a sequential keras model with two hidden layers with widths hw1 and hw2\"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_layer_width, activation=\"relu\", kernel_regularizer=l2(l2_penalty)\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_layer_width, activation=\"relu\", kernel_regularizer=l2(l2_penalty)\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Keras model to a sklearn compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_sklearn_classifier(build_fn):\n",
    "    \"\"\"Returns an sklearn compatible format of the Keras Classifier\"\"\"\n",
    "    return KerasClassifier(build_fn=build_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform n-fold cross validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ct.electron(executor=ssh_executor)\n",
    "@ct.electron\n",
    "def cross_validate(\n",
    "    model,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    n_folds: int = 3,\n",
    "    model_fit_params={\"epochs\": 10, \"batch_size\": 128, \"verbose\": 0},\n",
    "):\n",
    "    scores = cross_val_score(\n",
    "        model, train_images, train_labels, cv=n_folds, fit_params=model_fit_params\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main cross validation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ct.electron(executor=ssh_executor)\n",
    "@ct.electron\n",
    "@ct.lattice\n",
    "def classifier_cv_workflow(\n",
    "    training_set,\n",
    "    training_labels,\n",
    "    hidden_layer_width: int,\n",
    "    dropout_rate: float = 0.0,\n",
    "    l2_penalty: float = 0.1,\n",
    "):\n",
    "    # Preprocess the dataset\n",
    "    scaled_train_images, scaled_test_images = scale_dataset(train_images, test_images)\n",
    "\n",
    "    # Build the keras model\n",
    "    classifier = get_sklearn_classifier(\n",
    "        build_keras_classifier(hidden_layer_width, dropout_rate, l2_penalty)\n",
    "    )\n",
    "\n",
    "    # cross validate classifier\n",
    "    scores = cross_validate(classifier, scaled_train_images, train_labels, n_folds=3)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatch a single iteration of the cross validation workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = load_dataset()\n",
    "dispatch_id = ct.dispatch(classifier_cv_workflow)(\n",
    "    train_images, train_labels, 10, 0.0, 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get cv scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = ct.get_result(dispatch_id, wait=True).result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classifier_cv_workflow` performs a 3 fold cross validation for a specific values for the parameters i.e.\n",
    "* `hidden_layer_width`: 10\n",
    "* `dropout_rate`: 0.0\n",
    "* `l2_penalty`: 0.1\n",
    "\n",
    "We can now embed the `classifier_cv_workflow` as a **sublattice** of a larger workflow that runs `classifier_cv_workflow` for a variety of hyper parameters. To this end, we create a new workflow `optimize_hyperparameters` that feeds randomly generated hyperparameter values into `classifier_cv_workflow`, gets the cross validates scores and saves the results in a dictionary. We use `sklearn.model_selection.ParameterSampler` to return parameter values from distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the parameter space to be randomly sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = {\n",
    "    \"dropout_rate\": uniform(0.0, 0.1),\n",
    "    \"l2_penalty\": uniform(0, 1),\n",
    "    \"hidden_layer_width\": randint(10, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This electron return the average cross validation score return by `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.electron\n",
    "def get_avg_model_score(cv_scores: List[float]):\n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ct.lattice\n",
    "def optimize_hyperparameters(train_images, train_labels, parameters):\n",
    "    result = []\n",
    "    for index, parameter in enumerate(parameters):\n",
    "        cv_scores = classifier_cv_workflow(\n",
    "            train_images,\n",
    "            train_labels,\n",
    "            hidden_layer_width=parameter[\"hidden_layer_width\"],\n",
    "            dropout_rate=parameter[\"dropout_rate\"],\n",
    "            l2_penalty=parameter[\"l2_penalty\"],\n",
    "        )\n",
    "        result.append(\n",
    "            {\"parameters\": parameter, \"score\": get_avg_model_score(cv_scores)}\n",
    "        )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispatch the hyperparameter optimization workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sampled_parameters = list(ParameterSampler(pgrid, n_iter=1))\n",
    "\n",
    "dispatch_id = ct.dispatch(optimize_hyperparameters)(\n",
    "    train_images, train_labels, random_sampled_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = ct.get_result(dispatch_id, wait=True).result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4604952b9ddbd260dd73fdb5cff5f484d6b1e3984d5eb828a72400f80bc93c07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
